{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-5V4HKT8M:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24bad93ef48>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-5V4HKT8M:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext # RDD API entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, helpful: array<bigint>, overall: double, reviewText: string, reviewTime: string, reviewerID: string, reviewerName: string, summary: string, unixReviewTime: bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_paths =['data/reviews_fashion.json', \n",
    "                'data/reviews_electronics.json',\n",
    "                'data/reviews_sports.json']\n",
    "\n",
    "inferred_reviews = spark.read.json(reviews_paths)\n",
    "inferred_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a variable and call it \"POSIX\". The assign it with a string value representing \n",
    "# the filepath pattern that will capture all three .json files\n",
    "\n",
    "POSIX = \"data/reviews_*.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_reviews = spark.read.json(POSIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, helpful: array<bigint>, overall: double, reviewText: string, reviewTime: string, reviewerID: string, reviewerName: string, summary: string, unixReviewTime: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(reviewerID,StringType,true),StructField(asin,StringType,true),StructField(reviewerName,StringType,true),StructField(helpful,ArrayType(IntegerType,true),true),StructField(reviewTime,StringType,true),StructField(overall,DoubleType,true),StructField(summary,StringType,true),StructField(unixReviewTime,LongType,true)))\n"
     ]
    }
   ],
   "source": [
    "# Export the modules\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define Schema\n",
    "REVIEWS_SCHEMA_DEF = StructType([\n",
    "        StructField('reviewerID', StringType(), True),\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('reviewerName', StringType(), True),\n",
    "        StructField('helpful', ArrayType(\n",
    "                IntegerType(), True), \n",
    "            True),\n",
    "#        review text column definition missing here\n",
    "        StructField('reviewTime', StringType(), True),\n",
    "        StructField('overall', DoubleType(), True),\n",
    "        StructField('summary', StringType(), True),\n",
    "        StructField('unixReviewTime', LongType(), True)\n",
    "    ])\n",
    "\n",
    "print(REVIEWS_SCHEMA_DEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is DataFrame[reviewerID: string, asin: string, reviewerName: string, helpful: array<int>, reviewTime: string, overall: double, summary: string, unixReviewTime: bigint]\n"
     ]
    }
   ],
   "source": [
    "#The reviewText field is not added to the schema here. What do you think will happen if we try to enforce this \n",
    "#schema to the reviews dataset?\n",
    "#Spark will enforce manual schema on the defined columns and ignore others\n",
    "#Spark will enforce manual schema on the defined columns and infer schema for others\n",
    "# Try it out\n",
    "reviews = spark.read.json(POSIX, schema=REVIEWS_SCHEMA_DEF)\n",
    "print(\"The answer is {}\".format(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(reviewerID,StringType,true),StructField(asin,StringType,true),StructField(reviewerName,StringType,true),StructField(helpful,ArrayType(IntegerType,true),true),StructField(reviewText,StringType,true),StructField(reviewTime,StringType,true),StructField(overall,DoubleType,true),StructField(summary,StringType,true),StructField(unixReviewTime,LongType,true)))\n"
     ]
    }
   ],
   "source": [
    "#Add reviewText field to the data schema (5 mins)\n",
    "# Define FULL_REVIEWS_SCHEMA_DEF here\n",
    "\n",
    "FULL_REVIEWS_SCHEMA_DEF = StructType([\n",
    "        StructField('reviewerID', StringType(), True),\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('reviewerName', StringType(), True),\n",
    "        StructField('helpful', ArrayType(\n",
    "                IntegerType(), True), \n",
    "            True),\n",
    "        StructField('reviewText', StringType(), True),\n",
    "        StructField('reviewTime', StringType(), True),\n",
    "        StructField('overall', DoubleType(), True),\n",
    "        StructField('summary', StringType(), True),\n",
    "        StructField('unixReviewTime', LongType(), True)\n",
    "    ])\n",
    "\n",
    "print(FULL_REVIEWS_SCHEMA_DEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.json(POSIX, schema=FULL_REVIEWS_SCHEMA_DEF)\n",
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame operations\n",
    "#Spark DataFrame API allow you to do multiple operations on the Data. The primary advantage of using the DataFrame API\n",
    "#is that you can do data transformations with the high level API without having to use Python. Using the high level API \n",
    "#has performance advantages.\n",
    "#  DataFrame API have functionality similar to that of Core RDD API. For example: \n",
    "#      map : foreach, Select\n",
    "#      filter : filter\n",
    "#      groupByKey, reduceByKey : groupBy\n",
    "#\n",
    "##Selecting Columns\n",
    "#You can use SELECT statement to select columns from your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
